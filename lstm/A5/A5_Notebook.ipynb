{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUFNk0BJAMZH"
   },
   "source": [
    "# Assignment 5: Extended Long Short-Term Memory (xLSTM)\n",
    "\n",
    "*Author:* Philipp Seidl\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment, we will explore the xLSTM architecture, a novel extension of the classic LSTM model. The paper can be found here: https://arxiv.org/abs/2405.04517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBfgx3oEAc3W"
   },
   "source": [
    "## Background\n",
    "Recurrent Neural Networks (RNNs), particularly LSTMs, have proven highly effective in various sequence modeling tasks. However, the emergence of Transformers, with their parallel processing capabilities, has shifted the focus away from LSTMs, especially in large-scale language modeling.\n",
    "The xLSTM architecture aims to bridge this gap by enhancing LSTMs with mechanisms inspired by modern LLMs (e.g. block-strucutre, residual connections, ...).  Further it introduces:\n",
    "- Exponential gating with normalization and stabilization techniques, which improves gradient flow and memory capacity.\n",
    "- Modifications to the LSTM memory structure, resulting in two variants:\n",
    "    - sLSTM: Employs a scalar memory with a scalar update rule and a new memory mixing technique through recurrent connections.\n",
    "    - mLSTM: Features a matrix memory, employs a covariance update rule, and is fully parallelizable, making it suitable for scaling.\n",
    "\n",
    "By integrating these extensions into residual block backbones, xLSTM blocks are formed, which can then be residually stacked to create complete xLSTM architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08ut_E9kAdpU"
   },
   "source": [
    "## Exercise 1: Environment Setup\n",
    "\n",
    "When working with new architectures or specialized frameworks, it's essential to correctly set up the environment to ensure reproducability. This exercise focuses on setting up the environment for working with the `xlstm` repository.\n",
    "\n",
    "1. Visit and clone the official repository: [https://github.com/NX-AI/xlstm](https://github.com/NX-AI/xlstm).  \n",
    "2. Set up the environment  \n",
    "3. Document your setup:  \n",
    "   - OS, Python version, Environment setup, CUDA version (if applicable), and GPU details.  \n",
    "   - Note any challenges you faced and how you resolved them. \n",
    "4. Submit your setup as a bash script using the IPython `%%bash` magic. Ensure it is reproducible.\n",
    "\n",
    "Getting mLSTM working only is fine (if you encounter issues with sLSTM cuda kernels)\n",
    "\n",
    "> **Note**: Depending on your system setup, you may need to adjust the `environment_pt220cu121.yaml` file, such as for the CUDA version. For this assignment, it is recommended to run it on GPUs. If you don't have one, consider using  [Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) or other online resources.\n",
    "\n",
    "> **Recommendations**: While the repository suggests using `conda`, we recommend using `mamba` or `micromamba` instead (way faster) (except if you are using colab). Learn more about them here: [https://mamba.readthedocs.io/en/latest/index.html](https://mamba.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Bdb5fIMaKea1",
    "outputId": "fbea5037-812c-41a1-bd42-79f4b9790cd5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#python3.11 -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install xlstm\n",
    "pip install --upgrade pip\n",
    "\n",
    "pip install -e .\n",
    "pip install mlstm_kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "4FejZLBoK_Lo",
    "outputId": "37eadcd9-3134-45df-da5c-0db1a267f332",
    "ExecuteTime": {
     "end_time": "2025-01-12T20:32:52.546181Z",
     "start_time": "2025-01-12T20:32:51.603877Z"
    }
   },
   "source": [
    "# Verify your installation of xLSTM:\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "use_slstm_kernels = False # set to True if you want to check if sLSTM cuda kernels are working\n",
    "\n",
    "xlstm_cfg = f\"\"\"\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: {'cuda' if use_slstm_kernels else 'vanilla'}\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 32\n",
    "num_blocks: 7\n",
    "embedding_dim: 64\n",
    "slstm_at: [] # empty = mLSTM only\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMBlockStack(cfg)\n",
    "\n",
    "x = torch.randn(4, 32, 64).to(DEVICE)\n",
    "xlstm_stack = xlstm_stack.to(DEVICE)\n",
    "y = xlstm_stack(x)\n",
    "y.shape == x.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbkUQdktAkeG"
   },
   "source": [
    "## Exercise 2: Understanding xLSTM Hyperparameters\n",
    "Explain key hyperparameters that influence the performance and behavior of the xLSTM architecture and explain how they influence total parameter count.\n",
    "The explanation should include: proj_factor, num_heads, act_fn, context_length, num_blocks, embedding_dim, hidden_size, dropout, slstm_at, qkv_proj_blocksize, conv1d_kernel_size. Also include how the matrix memory size of mLSTM is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "#proj_factor: scaling factor for the hidden dimensions in the feedforward layers relative to the input dimensions\n",
    "##    Influence: Higher proj_factor increases the number of parameters in the feedforward layers â€“ \n",
    "#       large number --> overfitting in small DS\n",
    "\n",
    "#num_heads: Number of attention heads. Each head learns independent attention patterns. \n",
    "#     increases number of params in attention linearly.\n",
    "\n",
    "#act_fn: the activation function, \n",
    "#    no effect on param count\n",
    "\n",
    "#context_length: \n",
    "#    length of the input sequence processed by the model. --> how much past information the model can consider. \n",
    "#    more context_length --> more compute. \n",
    "#    No direct effect on \n",
    "\n",
    "\n",
    "#num_blocks: \n",
    "\n",
    "\n",
    "#embedding_dim: \n",
    "#     The size of the embedding vector for input tokens.\n",
    "##    improves the model's ability to learn detailed patterns but increases memory usage and parameter count \n",
    "##    (how much increase in param count?)\n",
    "\n",
    "#hidden_size:\n",
    "#   hidden layers\n",
    "##  similar to embedding, more hidden layer, detailed learning, more memory usage\n",
    "##  increase number of params\n",
    "\n",
    "#drop_out:\n",
    "# The probability of dropping neurons during training for regularization.\n",
    "# Reduce overfitting --> adding noise to training process\n",
    "# no imapct on param count\n",
    "\n",
    "\n",
    "# slstm_at:\n",
    "##  number of sLSTM layers\n",
    "\n",
    "\n",
    "#qkv_proj_blocksize:\n",
    "#    Size of the blocks used for QKV projection in attention \n",
    "#   \n",
    "\n",
    "\n",
    "#conv1d_kernel_size:\n",
    "#    The kernel size of the convolutional layers in LSTM components\n",
    "\n",
    "\n",
    "#Matrix Memory Size in mLSTM\n",
    "#   ???\n",
    "\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4rSyOdnAv6r"
   },
   "source": [
    "## Exercise 3: Train an xLSTM model on the Trump Dataset from the previous exercise\n",
    "Your task is to train an xLSTM model on the Trump Dataset from the previous exercise. \n",
    "- The goal is to achieve an average validation loss $\\mathcal{L}_{\\text{val}} < 1.35$. \n",
    "- You do not need to perform an extensive hyperparameter search, but you should document your runs. Log your runs with used hyperparameters using tools like wandb, neptune, mlflow, ... or a similar setup. Log training/validation loss and learning rate over steps as well as total trainable parameters of the model for each run.\n",
    "- You can use the training setup from the previous exercises or any setup of your choice using high level training libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "WcX8FAV8DjtG",
    "outputId": "cbd2f947-f8cc-4cbf-b24f-a6eb14091ae5"
   },
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVADqjO1A9kI"
   },
   "source": [
    "## Exercise 4: Utilizing a Pretrained Model (Bonus)\n",
    "\n",
    "Foundation Models, those pretrained on large amounts of data are more and more important. We can use those models and fine-tune them on our dataset, rather then training them from scratch.\n",
    "Here are the things to consider:\n",
    "\n",
    "- Model Selection: Choose a pretrained language model from an online repository. Hint: You can explore platforms like Hugging Face (huggingface.co), which host numerous pretrained models.\n",
    "\n",
    "- Dataset: Use the Trump dataset with the same training and validation split as in previous exercises. You do not need to use character tokenization.\n",
    "\n",
    "- Performance Evaluation: Evaluate the performance of the pretrained model on the validation set before and during fine-tuning. Report average-CE-loss as well as an example generated sequence with the same prompt for each epoch.\n",
    " \n",
    "- Fine-tuning: Adjust the learning rate, potentially freeze some layers, train for a few epochs with a framework of your choice (e.g. [lightning](https://lightning.ai/docs/pytorch/stable/), [huggingface](https://huggingface.co/models), ...)\n",
    "\n",
    "- Computational Resources: Be mindful of the computational demands of pretrained models. You might need access to GPUs. Try to keep the model size at a minimum and go for e.g. distilled versions or other small LMs\n",
    "\n",
    "- Hyperparameter Tuning: You can experiment with different learning rates and potentially other hyperparameters during fine-tuning but no need to do this in depth\n",
    "\n",
    "By completing this exercise, you will gain experience with utilizing pretrained models, understanding their capabilities, and the process of fine-tuning. Decreasing the validation loss can be seen a success for this exercise.\n",
    "\n",
    "> **Note**: This is a standalone exercise and doesn't build upon the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqv4tH69Ab0X"
   },
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
