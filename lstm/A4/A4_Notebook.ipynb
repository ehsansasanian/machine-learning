{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "id": "d28afb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:09:00.961344Z",
     "start_time": "2024-12-15T17:09:00.101968Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "       x: Input tensor of shape (N, T, D),\n",
    "          where N = batch size, T = sequence length, D = hidden size.\n",
    "       \"\"\"\n",
    "        N, T, D = x.shape\n",
    "        \n",
    "        # ensure hidden size matches input size\n",
    "        assert D == self.hidden_size, \"Input hidden size does not match initialized hidden size.\"\n",
    "\n",
    "        Q = self.W_Q(x)  # Shape: (N, T, D)\n",
    "        K = self.W_K(x)  # Shape: (N, T, D)\n",
    "        V = self.W_V(x)  # Shape: (N, T, D)\n",
    "\n",
    "        scale = 1 / (D ** 0.5)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device) \n",
    "        scores = scores.masked_fill(causal_mask == 1, float('-inf')) # set the future tokens to -inf for softmax\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # Shape: (N, T, T)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)  # Shape: (N, T, D)\n",
    "\n",
    "        return output\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ddee2d25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:18:05.390120Z",
     "start_time": "2024-12-15T17:18:05.375002Z"
    }
   },
   "source": [
    "class MultiHeadCasualSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert hidden_size % n_head == 0, \"hidden_size must be divisible by n_head\"\n",
    "        self.head_dim = hidden_size // n_head\n",
    "\n",
    "        # Learnable weight matrices for Q, K, V\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (N, T, D),\n",
    "           where N = batch size, T = sequence length, D = hidden size.\n",
    "        \"\"\"\n",
    "        N, T, D = x.shape\n",
    "\n",
    "        # Ensure the hidden size matches input dimensionality\n",
    "        assert D == self.hidden_size, \"Input hidden size does not match initialized hidden size.\"\n",
    "\n",
    "        # Compute Q, K, V matrices and split into heads\n",
    "        Q = self.W_Q(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)  # Shape: (N, n_head, T, head_dim)\n",
    "        K = self.W_K(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)  # Shape: (N, n_head, T, head_dim)\n",
    "        V = self.W_V(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)  # Shape: (N, n_head, T, head_dim)\n",
    "\n",
    "        # Compute scaled dot-product attention scores\n",
    "        scale = 1 / (self.head_dim ** 0.5)  # Scaling factor\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale  # Shape: (N, n_head, T, T)\n",
    "\n",
    "        # Apply causal mask\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).to(x.device)  # Upper triangular mask\n",
    "        scores = scores.masked_fill(causal_mask == 1, float('-inf'))\n",
    "\n",
    "        # Apply softmax to scores (row-wise)\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # Shape: (N, n_head, T, T)\n",
    "        attention_weights = self.dropout(attention_weights)  # Apply dropout\n",
    "\n",
    "        # Weighted sum of values (V)\n",
    "        output = torch.matmul(attention_weights, V)  # Shape: (N, n_head, T, head_dim)\n",
    "\n",
    "        # Concatenate heads and project back to hidden size\n",
    "        output = output.transpose(1, 2).contiguous().view(N, T, D)  # Shape: (N, T, D)\n",
    "        output = self.W_O(output)  # Output projection\n",
    "        output = self.dropout(output)  # Apply dropout\n",
    "\n",
    "        return output"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=5, bias=False)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        \"\"\"\n",
    "        Initializes a 2-layer feedforward network.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size (int): The size of the input and output features.\n",
    "            dropout (float): Dropout probability to be applied to the output.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Define the two linear layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 4 * hidden_size)  # First layer expands to 4*hidden_size\n",
    "        self.fc2 = nn.Linear(4 * hidden_size, hidden_size)  # Second layer reduces back to hidden_size\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, ..., hidden_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, ..., hidden_size).\n",
    "        \"\"\"\n",
    "        # Apply the first linear layer and GELU activation\n",
    "        x = self.gelu(self.fc1(x))\n",
    "\n",
    "        # Apply the second linear layer and dropout\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        \"\"\"\n",
    "        A transformer block with layer normalization, multi-head attention, and MLP.\n",
    "        \"\"\"\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Components of the Block\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)  # LayerNorm before attention\n",
    "        self.attn = MultiHeadCasualSelfAttention(hidden_size, n_head, dropout)  # Multi-head attention\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)  # LayerNorm before MLP\n",
    "        self.mlp = MLP(hidden_size, dropout)  # Feedforward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with residual connections.\n",
    "        \"\"\"\n",
    "        # Residual connection from input to attention output\n",
    "        attn_output = self.attn(self.ln1(x))  # Apply LayerNorm and attention\n",
    "        x = x + attn_output  # Add residual\n",
    "\n",
    "        # Residual connection from attention output to MLP output\n",
    "        mlp_output = self.mlp(self.ln2(x))  # Apply LayerNorm and MLP\n",
    "        x = x + mlp_output  # Add residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 250 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger training batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 64 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 4 # number of layers\n",
    "n_head = 4 # number of attention heads\n",
    "hidden_size = 128 # layer size\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.99 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "min_lr = 1e-4 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504f763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
