{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsKjl5AaRrF7"
      },
      "source": [
        "# Assignment 1: Introduction to the Fully Recurrent Network\n",
        "\n",
        "*Author:* Thomas Adler\n",
        "\n",
        "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
        "\n",
        "\n",
        "## Exercise 1: Numerical stability of the binary cross-entropy loss function\n",
        "\n",
        "We will use the binary cross-entropy loss function to train our RNN, which is defined as\n",
        "$$\n",
        "L_{\\text{BCE}}(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y),\n",
        "$$\n",
        "where $y$ is the label and $\\hat y$ is a prediction, which comes from a model (e.g. an RNN) and is usually sigmoid-activated, i.e., we have\n",
        "$$\n",
        "\\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "The argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit $z$ (instead of the prediction $\\hat y$) and incorporate the sigmoid activation into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions $L_{\\text{BCE}}(\\hat y, y)$ and $\\sigma(z)$ into one function $L(z, y) = L_{\\text{BCE}}(\\sigma(z), y)$.\n",
        "\n",
        "*Hint: Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$ and argue why the right-hand side is numerically more stable. Finally, express $L(z,y)$ in terms of that form.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCh4bq-7RrF9"
      },
      "source": [
        "########## YOUR SOLUTION HERE ##########\n",
        "\n",
        "\n",
        "Combining Sigmoid and BCE: Substitude $\\hat y = \\sigma(z)$ into the BCE loss func:\n",
        "\n",
        "$$\n",
        "L(z,y) = -y\\log(\\sigma(z)) - (1-y)\\log(1-\\sigma(z))\n",
        "$$\n",
        "\n",
        "applying:\n",
        "\n",
        "$$\n",
        "\\log(\\sigma(z)) = -\\log(1+e^{-z})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log(1-\\sigma(z)) = -\\log(1+e^z)\n",
        "$$\n",
        "\n",
        "which resulted from applying log to: $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "Substitude, the BCE becomes:\n",
        "\n",
        "$$\n",
        "L(z,y) = y\\log(1+e^{-z}) + (1-y)\\log(1+e^z)\n",
        "$$\n",
        "\n",
        "To avoid issues relate to computing $\\log (1-\\hat y)$ and $\\log(\\hat y)$ when $z$ is extremely small or larg it can cause underflow or overflow issues in computations. To do so, instead of calculating loss based on $\\hat y$, the loss should be calculated via logit z directly.\n",
        "\n",
        "To prove the hint:\n",
        "\n",
        "$$\n",
        "\\log(1 + e^z) = \\log(1 + e^{-|z|}) + \\max(0, z),\n",
        "$$\n",
        "\n",
        "There are two cases:\n",
        "\n",
        "\n",
        "1. **For $z \\geq 0$:**\n",
        "\n",
        "$$\\log(1 + e^z) = z + \\log(1 + e^{-z}),$$\n",
        "and since $\\max(0, z) = z$, the equation holds.\n",
        "\n",
        "2. **For $z < 0$:**\n",
        "\n",
        "$$\\log(1 + e^z) = \\log(1 + e^z),$$\n",
        "\n",
        "Therefore, the numerically stable version of the binary cross-entropy loss function is the below expression which we proved in the begining:\n",
        "\n",
        "$$L(z, y) = y \\log(1 + e^{-z}) + (1 - y) \\log(1 + e^z).$$\n",
        "\n",
        "\n",
        "Side note:\n",
        "\n",
        "Instead of explicitly calculating $\\hat y = \\sigma(z)$ we can calculate the loss in terms of $z$ (the logit) directly for numerical stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1xV0dFERrF9"
      },
      "source": [
        "## Exercise 2: Derivative of the loss\n",
        "\n",
        "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4R-n4GORrF9"
      },
      "source": [
        "########## YOUR SOLUTION HERE ##########\n",
        "\n",
        "The binary cross-entropy loss function:\n",
        "\n",
        "$$\n",
        "L(z, y) = -\\left[ y \\log(\\sigma(z)) + (1 - y) \\log(1 - \\sigma(z)) \\right]\n",
        "$$\n",
        "\n",
        "Substitude the sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ to the BCE loss function:\n",
        "\n",
        "$$\n",
        "L(z, y) = -\\left[ y \\log\\left( \\frac{1}{1 + e^{-z}} \\right) + (1 - y) \\log\\left( 1 - \\frac{1}{1 + e^{-z}} \\right) \\right]\n",
        "$$\n",
        "\n",
        "## Simplification:\n",
        "\n",
        "I. Simplifying the First Term\n",
        "\n",
        "$$\n",
        "y \\log\\left( \\frac{1}{1 + e^{-z}} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\implies y \\log\\left( \\frac{1}{1 + e^{-z}} \\right) = -y \\log(1 + e^{-z})\n",
        "$$\n",
        "\n",
        "II. Simplifying the Second Term\n",
        "\n",
        "\n",
        "$$\n",
        "(1 - y) \\log\\left( 1 - \\frac{1}{1 + e^{-z}} \\right)\n",
        "$$\n",
        "\n",
        "inside the logarithm:\n",
        "\n",
        "$$\n",
        "1 - \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\implies 1 - \\frac{1}{1 + e^{-z}} = \\frac{e^{-z}}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\implies (1 - y) \\log\\left( 1 - \\frac{1}{1 + e^{-z}} \\right) = (1 - y) \\log\\left( \\frac{e^{-z}}{1 + e^{-z}} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\implies \\log\\left( \\frac{e^{-z}}{1 + e^{-z}} \\right) = \\log(e^{-z}) - \\log(1 + e^{-z}) = -z - \\log(1 + e^{-z})\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\implies (1 - y) \\log\\left( 1 - \\frac{1}{1 + e^{-z}} \\right) = (1 - y)(-z - \\log(1 + e^{-z}))\n",
        "$$\n",
        "\n",
        "Expanding this expression:\n",
        "\n",
        "$$\n",
        "\\implies (1 - y)(-z - \\log(1 + e^{-z})) = -(1 - y)z - (1 - y)\\log(1 + e^{-z})\n",
        "$$\n",
        "\n",
        "### Combining Both Parts\n",
        "\n",
        "\n",
        "$$\n",
        "\\implies L(z, y) = (1 - y)z + \\log(1 + e^{-z})\n",
        "$$\n",
        "\n",
        "\n",
        "To find the derivative of the loss with respect to the logit $z$ (using the chain rule):\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z}\n",
        "$$\n",
        "\n",
        "The partial derivative of the loss with respect to the sigmoid output $p$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial p} = -\\frac{y}{p} + \\frac{1 - y}{1 - p}\n",
        "$$\n",
        "\n",
        "The derivative of the sigmoid function with respect to $z$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p}{\\partial z} = \\sigma(z) \\cdot (1 - \\sigma(z)) = p(1 - p)\n",
        "$$\n",
        "\n",
        "The derivative of the loss function $L(z, y)$ with respect to the logit ($z$) is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z} = \\sigma(z) - y\n",
        "$$\n",
        "\n",
        "### Intuition Behind the Result (Side note)\n",
        "\n",
        "* The derivative $\\sigma(z) - y $ tells us the difference between the predicted probability $\\sigma(z)$ and the true label $y$.\n",
        "* This value is often referred to as the **error term** in the context of gradient descent: it represents how far off the prediction is from the actual label.\n",
        "* If the prediction $\\sigma(z)$ is greater than the actual value $y$, the derivative will be positive, suggesting that the model needs to reduce the logit value to minimize the loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_KMwqsxRrF-"
      },
      "source": [
        "## Exercise 3: Initializing the network\n",
        "Consider the fully recurrent network\n",
        "$$\n",
        "s(t) = W x(t) + R a(t-1) \\\\\n",
        "a(t) = \\tanh(s(t)) \\\\\n",
        "z(t) = V a(t) \\\\\n",
        "\\hat y(t) = \\sigma(z(t))\n",
        "$$\n",
        "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$.\n",
        "\n",
        "*Compared to the lecture notes we choose $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$. Further, we introduced an auxiliary variable $z(t)$ and transposed the weight matrices.*\n",
        "\n",
        "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8yyO0dWRrF-",
        "outputId": "3854c1b1-b02b-46fb-8a9e-d832e7bcb1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W shape: (5, 3)\n",
            "R shape: (5, 5)\n",
            "V shape: (1, 5)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "class Obj(object):\n",
        "    pass\n",
        "\n",
        "model = Obj()\n",
        "T, D, I, K = 10, 3, 5, 1\n",
        "\n",
        "def init(model, D, I, K):\n",
        "    # Using uniform(lowerBound, upperBound, shape)\n",
        "\n",
        "    # Initialize W with shape (5x3)\n",
        "    model.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
        "    # Initialize R with shape (5x5)\n",
        "    model.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
        "    # Initialize D with shape (1x5)\n",
        "    model.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
        "\n",
        "Obj.init = init\n",
        "model.init(D, I, K)\n",
        "\n",
        "print(\"W shape:\", model.W.shape)\n",
        "print(\"R shape:\", model.R.shape)\n",
        "print(\"V shape:\", model.V.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBMabafGRrF-"
      },
      "source": [
        "## Exercise 4: The forward pass\n",
        "Implement the forward pass for the fully recurrent network for sequence classification (many-to-one mapping). To this end, write a function `forward` that takes a `model`, a sequence of input vectors `x`, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the fully recurrent network and evaluate the (numerically stabilized) binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the logit $z(T)$ into `model.a` and `model.z`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "wZbmFmuYRrF-",
        "outputId": "511011b2-32df-4253-941f-b47dd7491303"
      },
      "outputs": [],
      "source": [
        "def forward(model, x, y):\n",
        "    T, D = x.shape  # Sequence length and input dimensionality\n",
        "\n",
        "    # Initialize hidden state\n",
        "    h = np.zeros((D,)) \n",
        "\n",
        "    # Initialize storage for activations\n",
        "    model.a = []\n",
        "\n",
        "    # Iterate over each time step\n",
        "    for t in range(T):\n",
        "        # Recurrent operation\n",
        "        h = np.tanh(np.dot(model.W_h, h) + np.dot(model.W_x, x[t]) + model.b)\n",
        "\n",
        "        # Store hidden activations\n",
        "        model.a.append(h)\n",
        "\n",
        "    # Final logit for the last time step\n",
        "    z_T = np.dot(model.W_out, h) + model.b_out\n",
        "    model.z = z_T\n",
        "\n",
        "    # Compute binary cross-entropy loss (with numerical stability)\n",
        "    loss = -y * np.log(sigmoid(z_T)) - (1 - y) * np.log(1 - sigmoid(z_T))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "Obj.forward = forward\n",
        "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRxVTY6QRrF-"
      },
      "source": [
        "## Exercise 5: The computational graph\n",
        "\n",
        "Visualize the computational graph of the fully recurrent network unfolded in time. The graph should show the functional dependencies of the nodes $x(t), a(t), z(t), L(z(t), y(t))$ for $t \\in \\{1, 2, 3\\}$. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.org/documentation/stable/tutorial.html). Make sure to arrange the nodes in a meaningful way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNSB3BE0RrF-"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "########## YOUR SOLUTION HERE ##########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyYeAiB5RrF_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
