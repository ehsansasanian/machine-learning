{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e442cc",
   "metadata": {},
   "source": [
    "# Assignment 3: Text processing with LSTM in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment you will a train an LSTM to generate text. To be able to feed text into (recurrent) neural networks we first have to choose a good representation. There are several options to do so ranging from simple character embeddings to more sophisticated approaches like [word embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) or [token embeddings](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a). We will use a character embedding in this assignment. \n",
    "\n",
    "Character embeddings work as follows. First we define an alphabet, a set of characters that we want to be able to represent. To feed a character into our network we use a one-hot vector. The dimension of this vector is equal to the size of our alphabet and the \"hot\" position indicates the character we want to represent. While this is logically a decent representation (all characters have the same norm, are orthogonal to one another, etc.) it is inefficient in terms of memory because we have to store a lot of zeros. In the first layer of our network we will multiply our one-hot vector with a weight matrix, i.e. we compute the preactivation by a matrix-vector product of the form $We_i$, where $e_i$ is the $i$-th canonical basis vector. This operation corresponds to selecting the $i$-th column of $W$. So an efficient implementation is to perform a simple lookup operation in $W$. This is how embedding layers work also for word or token embeddings. They are learnable lookup tables. \n",
    "\n",
    "## Exercise 1: Encoding characters\n",
    "\n",
    "Write a class `Encoder` that implements the methods `__init__` and `__call__`. The method `__init__` takes a string as argument that serves as alphabet. The method `__call__` takes one argument. If it is a string then it should return a sequence of integers as `torch.Tensor` of shape  representing the input string. Each integer should represents a character of the alphabet. The alphabet consists of the characters matched by the regex `[a-z0-9 .!?]`. If the input text contains characters that are not in the alphabet, then `__call__` should either remove them or map them to a corresponding character that belongs to the alphabet. If the argument is a `torch.Tensor`, then the method should return a string representation of the input, i.e. it should function as decoder. "
   ]
  },
  {
   "cell_type": "code",
   "id": "17f16ae5",
   "metadata": {},
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "class Encoder:\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(alphabet)}\n",
    "        self.index_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.regex = re.compile(f\"[{re.escape(alphabet)}]\")\n",
    "\n",
    "    def __call__(self, input_data):\n",
    "        if isinstance(input_data, str):\n",
    "            filtered_txt = ''.join(self.regex.findall(input_data))\n",
    "            indices = [self.char_to_idx[char] for char in filtered_txt]\n",
    "            return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        elif isinstance(input_data, torch.Tensor):\n",
    "            if input_data.dim() != 1:\n",
    "                raise ValueError(\"Input tensor must be 1-dimensional\")\n",
    "            decoded_text = ''.join(self.index_to_char[idx.item()] for idx in input_data)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a torch.Tensor\")\n",
    "\n",
    "\n",
    "# Test:\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 .!?\"\n",
    "alphabet_encoder = Encoder(alphabet)\n",
    "\n",
    "# Encoding a string\n",
    "text = \"hello world!\"\n",
    "print(f'Text: {text}')\n",
    "encoded_text = alphabet_encoder(text)\n",
    "print(\"Encoded Text: \", encoded_text)\n",
    "\n",
    "# Decoding a tensor\n",
    "decoded = alphabet_encoder(encoded_text)\n",
    "print(\"Decoded Text: \", decoded)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a819873",
   "metadata": {},
   "source": [
    "## Exercise 2: Pytorch Dataset\n",
    "\n",
    "Write a class `TextDataset` that derives from `torch.utlis.data.Dataset`. It should wrap a text file and utilize it for training with pytorch. Implement the methods `__init__`, `__len__`, `__getitem__`. The method `__init__` should take a path to a text file as string and an integer `l` specifying the length of one sample sequence. The method `__len__` takes no arguments and should return the size of the dataset, i.e. the number of sample sequences in the dataset. The method `__getitem__` should take an integer indexing a sample sequence and should return that sequence as a `torch.Tensor`. The input file can be viewed as one long sequence. The first sample sequence consists of the characters at positions `0..l-1` in the input file. The second sequence consists of the characters at positions `l..2*l-1` and so on. That is, the samples of our dataset are non-overlapping sequences. The last incomplete sequence may be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "id": "9df917ce",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, path: str, length: int, encoder: Encoder = None):\n",
    "        self.encoder = encoder\n",
    "        self.sequence_length = length\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.data = file.read()\n",
    "        self.num_samples = len(self.data) // self.sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index >= self.num_samples:\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "\n",
    "        start_idx = index * self.sequence_length\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        seq = self.data[start_idx:end_idx]\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            return self.encoder(seq)\n",
    "        \n",
    "        return torch.tensor([ord(char) for char in seq], dtype=torch.int64)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42e5724f",
   "metadata": {},
   "source": [
    "## Exercise 3: The Model\n",
    "\n",
    "Write a class `NextCharLSTM` that derives from `torch.nn.Module` and takes `alphabet_size`, the `embedding_dim`, and the `hidden_dim` as arguments. It should consist of a `torch.nn.Embedding` layer that maps the alphabet to embeddings, a `torch.nn.LSTM` that takes the embeddings as inputs and maps them to hidden states, and a `torch.nn.Linear` output layer that maps the hidden states of the LSTM back to the alphabet. Implement the methods `__init__` that sets up the module and `forward` that takes an input sequence and returns the logits (i.e. no activation function on the output layer) of the model prediction at every time step. "
   ]
  },
  {
   "cell_type": "code",
   "id": "459fe907",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NextCharLSTM(nn.Module):\n",
    "    def __init__(self, alphabet_size, embedding_dim, hidden_dim):\n",
    "        super(NextCharLSTM, self).__init__()\n",
    "\n",
    "        # Embedding layer to map the alphabet to embeddings\n",
    "        self.embedding = nn.Embedding(alphabet_size, embedding_dim)\n",
    "        # LSTM layer to process the embeddings and produce hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Output layer to map the hidden states back to the alphabet logits\n",
    "        self.fc = nn.Linear(hidden_dim, alphabet_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # embeddings for the input sequence\n",
    "        embeddings = self.embedding(input_seq)\n",
    "\n",
    "        # Pass embeddings through the LSTM\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "\n",
    "        # Apply the fully connected layer to LSTM outputs\n",
    "        logits = self.fc(lstm_out)\n",
    "\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd7cb10d",
   "metadata": {},
   "source": [
    "## Exercise 4: Training/Validation Epoch\n",
    "\n",
    "Write a function `epoch` that takes a `torch.utils.data.DataLoader`, a `NextCharLSTM`, and a `torch.optim.Optimizer` as arguments, where the last one might be `None`. If the optimizer is `None`, then the function should validate the model. Otherwise it should train the model for next-character prediction in the many-to-many setting. That is, given a sequence `x` of length `l`, the input sequence is `x[:l-1]` and the corresponding target sequence is `x[1:]`. The function should perform one epoch of training/validation and return the loss values of each mini batch as a numpy array. Use the cross-entropy loss function for both training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "id": "13f33250",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def epoch(data_loader: DataLoader, model: NextCharLSTM, optimizer: Optimizer = None) -> np.ndarray:\n",
    "    if optimizer is None:\n",
    "        model.eval()  # Validation mode\n",
    "    else:\n",
    "        model.train()  # Training mode\n",
    "\n",
    "    # List to store loss values for each mini-batch\n",
    "    batch_losses = []\n",
    "\n",
    "    # Cost function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        inputs: Tensor = batch[:, :-1]  # Input sequence x[:l-1]\n",
    "        targets: Tensor = batch[:, 1:]  # Target sequence x[1:]\n",
    "\n",
    "        # Move inputs and targets to the same device as the model\n",
    "        inputs, targets = inputs.to(model.embedding.weight.device), targets.to(model.embedding.weight.device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(inputs)  # Logits shape: (batch_size, seq_length-1, alphabet_size)\n",
    "\n",
    "        # Reshape logits and targets for cross-entropy loss\n",
    "        logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_length-1, alphabet_size)\n",
    "        targets = targets.view(-1)  # Shape: (batch_size * seq_length-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, targets)\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        if optimizer is not None:\n",
    "            # Training step\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "    return np.array(batch_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3cb856c",
   "metadata": {},
   "source": [
    "## Exercise 5: Model Selection\n",
    "\n",
    "Usually, we would now train and validate our model on a grid of with different hyperparameters to see which setting performs best. However, this is pretty expensive in terms of compute so we will provide you with a setting that should work quite well. Train your model for 30 epochs using `torch.optim.Adam`. Validate your model after every epoch and persist the model that performs best on the validation set using `torch.save`. Visualize and discuss the training and validation progress. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_and_validate(\n",
    "        model: NextCharLSTM,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        epochs: int = 30,\n",
    "        lr: float = 1e-3,\n",
    "        save_path: str = \"best_model.pth\"\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch_num in range(1, epochs + 1):\n",
    "        print(f\"Epoc {epoch_num} of {epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = epoch(train_loader, model, optimizer)\n",
    "        train_loss_mean = np.mean(train_loss)\n",
    "        train_losses.append(train_loss_mean)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = epoch(val_loader, model, optimizer = None)\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        val_losses.append(val_loss_mean)\n",
    "\n",
    "        print(f\"  Training Loss: {train_loss_mean:.4f}\")\n",
    "        print(f\"  Validation Loss: {val_loss_mean:.4f}\")\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if val_loss_mean < best_val_loss:\n",
    "            best_val_loss = val_loss_mean\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Return the recorded losses for visualization\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot training and validation losses over epochs.\n",
    "    \n",
    "    :param train_losses: List of training losses\n",
    "    :param val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ],
   "id": "623fc06cc8a2b966",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8987ae83",
   "metadata": {},
   "source": [
    "sequence_length = 100\n",
    "batch_size = 256\n",
    "embedding_dim = 8\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "alphabet_size = 30\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TextDataset(\"trump/trump_train.txt\", sequence_length, alphabet_encoder)\n",
    "val_dataset = TextDataset(\"trump/trump_train.txt\", sequence_length, alphabet_encoder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the model\n",
    "model = NextCharLSTM(alphabet_size, embedding_dim, hidden_dim)\n",
    "model.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Train and validate the model\n",
    "train_losses, val_losses = train_and_validate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    alphabet_size, \n",
    "    num_epochs, \n",
    "    learning_rate\n",
    ")\n",
    "\n",
    "# Visualize training and validation progress\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62c68d0c",
   "metadata": {},
   "source": [
    "## Exercise 6: Top-$k$ Accuracy\n",
    "\n",
    "Write a function `topk_accuracy` that takes a list of integers $k$, a model, and a data loader and returns the top-$k$ accuracy of the model on the given data set for each $k$. A sample is considered to be classified correctly if the true label appears in the top-$k$ classes predicted by the model. Then load the best model from the previous exercise using `torch.load` and plot its top-$k$ accuracy as a function of $k$ for all possible values of $k$. Discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "id": "fe1f70cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:05:40.136336Z",
     "start_time": "2024-12-01T20:05:40.124408Z"
    }
   },
   "source": [
    "\n",
    "def topk_accuracy(k_list, model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute the top-k accuracy of a model on a dataset for multiple values of k.\n",
    "\n",
    "    Args:\n",
    "        k_list (list of int): List of k values for top-k accuracy.\n",
    "        model (torch.nn.Module): Trained model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for the dataset to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each k to its corresponding top-k accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Initialize counters for each k\n",
    "    correct_counts = {k: 0 for k in k_list}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "            # Get top-k predictions\n",
    "            for k in k_list:\n",
    "                top_k_preds = torch.topk(probs, k, dim=-1).indices\n",
    "                # Check if true labels are in top-k predictions\n",
    "                correct_counts[k] += (top_k_preds == targets.unsqueeze(-1)).any(dim=-1).sum().item()\n",
    "\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    # Compute accuracy for each k\n",
    "    topk_accuracies = {k: correct_counts[k] / total_samples for k in k_list}\n",
    "    return topk_accuracies\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:06:36.340430Z",
     "start_time": "2024-12-01T20:06:36.275405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the best model\n",
    "best_model = NextCharLSTM(alphabet_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Define k values for evaluation\n",
    "k_values = list(range(1, alphabet_size + 1))\n",
    "\n",
    "# Compute top-k accuracy\n",
    "topk_accuracies = topk_accuracy(k_values, best_model, val_loader)\n",
    "\n",
    "# Plot top-k accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, [topk_accuracies[k] for k in k_values], marker='o')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Top-k Accuracy\")\n",
    "plt.title(\"Top-k Accuracy vs k\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "59c84424e975b1a5",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [84] at entry 0 and [90] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m k_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, alphabet_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Compute top-k accuracy\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m topk_accuracies \u001B[38;5;241m=\u001B[39m \u001B[43mtopk_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Plot top-k accuracy\u001B[39;00m\n\u001B[1;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n",
      "Cell \u001B[0;32mIn[39], line 21\u001B[0m, in \u001B[0;36mtopk_accuracy\u001B[0;34m(k_list, model, data_loader)\u001B[0m\n\u001B[1;32m     18\u001B[0m total_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 21\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Forward pass\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/Documents/ehsan/masters/assignments/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [84] at entry 0 and [90] at entry 1"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "04961062",
   "metadata": {},
   "source": [
    "## Exercise 7: Gumbel-Max Character Sampling\n",
    "\n",
    "In this exercise we utilize the trained network to generate novel text. To do this, take some string of seed text, which you can choose freely, and feed it to the network. For each subsequent character, the model outputs logits $z = (z_1, \\dots, z_K)^\\top$, where $K$ is the alphabet size. \n",
    "\n",
    "Use the Gumbel-Max trick to sample from the categorical distribution parameterized by \n",
    "$$\n",
    "\\pi_k = \\frac{e^{z_k / \\tau}}{\\sum_{j=1}^K e^{z_j / \\tau}} \\quad \\text{where} \\quad \\tau > 0 \n",
    "$$\n",
    "is the temperature. For $\\tau \\to 0$ we approach the one-hot distribution, whereas for $\\tau \\to \\infty$ we approach the uniform distribution. The Gumbel-Max trick says that the random variable \n",
    "$$\n",
    "Y = \\arg \\max_{k \\in 1, \\dots, K} (z_k / \\tau + \\xi_k) \n",
    "$$\n",
    "follows a categorical distribution parameterized by $\\pi_1, \\dots, \\pi_K$, where $\\xi_k$ is drawn independently from the standard Gumbel distribution.\n",
    "\n",
    "Implement next-character sampling using the Gumbel-Max trick. Try out different values of $\\tau$ and see which work best. "
   ]
  },
  {
   "cell_type": "code",
   "id": "1a92f049",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0efc114",
   "metadata": {},
   "source": [
    "## Exercise 8: Huffman Coding using LSTM\n",
    "\n",
    "*Thanks to Philipp Renz who had this idea.*\n",
    "\n",
    "The Huffman code is an algorithm to compress data. It encodes symbols with different lengths depending on their frequencies. It assigns a short code to frequent symbols and a longer code to rare symbols to minimize the average code length. We provide you with an implementation that given a list of frequencies `freqs` returns a list of their respective binary codes as strings in the same order. In fact, `freqs` may contain any real numbers. \n",
    "\n",
    "With a model that predicts the next symbol we can achieve even shorter codes. At every time step we can use the predicted probabilities as frequencies for the Huffman code. That is, we use a new code at every time step. This code is governed by the model's belief what the next symbol will be. If the model predictions are good, we will mostly use very short codes.\n",
    "\n",
    "First, determine the average code length per symbol on the validation set using frequencies determined on the training set. \n",
    "Then, use the prediction probabilities of your trained LSTM and determine the average code length per symbol on the validation set using an adaptable code. Add a temperature to the softmax and tune it. How many bits per symbol can you save by using the LSTM and what is the optimal temperature? "
   ]
  },
  {
   "cell_type": "code",
   "id": "57e483c8",
   "metadata": {},
   "source": [
    "from heapq import heapify, heappop, heappush\n",
    "\n",
    "\n",
    "def huffman_code(freqs):\n",
    "    \"\"\"This function turns a list of frequencies into a Huffman code. \"\"\"\n",
    "    heap = list(zip(freqs, [(i,) for i in range(len(freqs))]))\n",
    "    heapify(heap)\n",
    "    code = [''] * len(freqs)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        freq0, idx0 = heappop(heap)\n",
    "        freq1, idx1 = heappop(heap)\n",
    "        heappush(heap, (freq0 + freq1, idx0 + idx1))\n",
    "\n",
    "        for i in idx0:\n",
    "            code[i] = '0' + code[i]\n",
    "\n",
    "        for i in idx1:\n",
    "            code[i] = '1' + code[i]\n",
    "\n",
    "    return code\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
